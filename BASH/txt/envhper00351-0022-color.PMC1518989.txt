FIocu
Strange
Brevw: Assessing
'U
The prospect of assessing human health
risks from exposure to chemical mixtures
looms as a nightmare for many scientists,
especially toxicologists charged with com-
ing up with the necessary basic data.
Indeed, exposure at a variety of levels to
large numbers of chemical compounds,
either concurrently or sequentially via mul-
tiple pathways, is the environmental reality
for just about everyone on the planet.
Mixtures ofchemicals are ubiquitous in
ground and surface water, in our air, food,
and drinking water, as well as in soil sur-
rounding leaking toxic waste disposal sites.
Examples of environmentally prevalent
chemical mixtures are cigarette smoke,
diesel and automobile exhaust, disinfection
by-products from chlorination, and dioxin
and dioxinlike compounds formed as by-
products of incomplete combustion ofhos-
pital and municipal waste.
Despite this potential for exposure to
environmental mixtures, the vast majority
of established exposure standards are for
single compounds. Moreover, the vast
majority of toxicology studies examine the
cancer and noncancer effects of single
chemicals. Currently, more than 95% of
the resources in toxicology are devoted to
single-chemical studies. "For most chemi-
cal mixtures and multiple chemical expo-
sures, adequate data on exposure and toxic-
ity are lacking," says toxicologist Victor J.
Feron, senior scientist with TNO
Nutrition and Food Research
Institute, in the Netherlands.
A number of factors may
account for this data shortfall.
Harold Zenick, deputy direc-
tor of EPA's Health Effects
Research Laboratory (HERL),
points to the issue of difficul-
ty. "There are those who argue
that [the question of chemical
mixtures] is too difficult a
topic to be undertaken in a
research venue. This is based
on the belief that it's difficult Harold Ze
enough for us to address all the cating the is
uncertainties associated with mixtures is
single chemical risk assess- multiple mec
hnic
ssue
the
char
ment." Mixtures research, Feron
says, adds a layer ofincreased com-
plexity to risk assessment because
of the potential for multiple
mechanisms functioning simulta-
neously. This, he explains, com-
plicates the ability to extrapolate
to other dose levels and exposure
scenarios, to other mixtures, and
to other species. "And given that
you may have either multiple
chemical exposures via single or
multiple pathways, you may also
be eliciting multiple effects. And what is
less apparent is whether those effects are
independent or interactive."
Choosing the Approach
Basic to the study of chemical mixtures is
the issue of what approach to take. A bot-
tom-up approach is typically aimed at
identifying mechanistic interactions of sim-
ple mixtures to predict their effects, while
top-down studies examine the effects of
complex mixtures to determine the under-
lying mechanisms. Strict adherents to the
bottom-up approach may be criticized for
lacking "real world" immediacy, which is
exposure to complex mixtures. Critics of
the top-down route say scientists could test
mixtures and their components forever.
"The challenge for toxicology is devel-
opment of a database necessary for the risk
assessment process for chemical mixtures,"
says HERL toxicologist Jane
< Ellen Simmons. "However,
i toxicity assessment by itself is
not a feasible approach. There
are quite simply too many
mixtures and multiple chemi-
cal exposures for us to realisti-
cally think we can assess the
toxicity of each of them."
Simmons, a HERL team
leader for chemical mixtures
and interactions, describes
how quickly an experimental
study of just three chemicals
of chempcal at five different dose levels
question of (including a zero dose) can
nisms. become very cumbersome and
problematic. It would require
125 treatment groups, she ex-
plains, and at 10 animals per
group would require 1,250
animals. Such a study would
only be able to look at toxici-
ty at a one time point and at
only one dosing regimen rele-
t vant to that exposure. "Given
there are too many mixtures
for toxicity assessment to be a
reasonable or viable ap-
proach, a realistic possibility
is the development ofmechanistic models,"
Simmons says. "The evaluation of mecha-
nisms can be incorporated into both top-
down and bottom-up approaches," she
adds. Thus, according to Simmons, under-
standing the mechanism ofaction based on
simple mixtures should lead to improve-
ment in assessing risks of complex mix-
tures. Likewise, top-down toxicological
evaluation of complex mixtures provides
not only valuable and similar information
for other mixtures, but also a context to
interpret mechanistic understandings based
on simple mixtures.
Lingua Franca
Inconsistent usage of certain key terms
within the toxicology literature has some-
times made communication of findings on
chemical mixtures problematic. This is
especially true in fields such as toxicology
and biostatistics where many synonyms
have been employed for certain words or
where there has been lack of agreement
over the precise meaning of terms. "Such
lack of communication and frequent anar-
chy creates a high baseline of confusion
within the scientific and regulatory com-
munities but also the general public," says,
Edward J. Calabrese ofthe School of Public
Health ofthe University ofMassachusetts.
Calabrese and others now propose
three fundamental classes of joint interac-
tion of chemicals, defined as follows:
* Additivity-the effect of a combination
is exactly what is expected. For example,
the combination of one chemical with a
toxicity level of 1, with another com-
Environmental Health Perspectives
is
142
a
M
0
i; Art %I.
pound also having a toxicity of 1 would
equal a toxicity level of 2. This general
classification of additivity implies noth-
ing about how the addition occurs.
* Synergy-a positive interaction such that
the response is greater than expected.
Simply put, a combination of two com-
pounds with individual toxicity levels of
1 might yield a toxicity level of 10, for
example.
* Antagonism-a negative interaction such
that the response is less than expected.
Here, the mixture of two compounds
with a toxicity level of 1 each might give
a toxicity level of 1.5.
Whether these suggested definitions will
take hold remains to be seen. Today,
throughout the literature and at confer-
ences, a plethora of terms are used, such as
"greater than additive" or "superadditivity,"
"less than additive" or "subadditivity," and
"potentiation," "augmentation," and "inde-
pendence." Calabrese wrote in Multiple
Chemical Interactions: "The time has come
to seek the lowest common denominator
around which most will agree. In such
cases, simplicity is often the path to greater
clarity and scientific sanity."
Additivity by Default
Associated with the three fundamental joint
interactions described above are three possi-
ble results in assessing risk ofchemical mix-
tures: overestimation, correct estimation,
and underestimation. How these results
might play out can be understood in terms
ofcurrent risk assessment practices.
Guidelines ofnational and international
organizations involved in setting exposure
standards typically suggest the use of sim-
ple "dose addition" or "response addition"
models for assessing the hazard of a chemi-
cal mixture. To derive a best estimate of
risk, EPA guidelines say it is preferable to
have a lot of toxicological data on the mix-
ture. In the absence of this information,
the initial default is to use data on a similar
mixture. However, such information is
rarely available. Risk is then estimated
based upon knowledge of the mixture's
known components. In the absence ofdata
to the contrary, the health risk ofany given
mixture is estimated by adding the risks of
the individual components. Thus, the
additivity default typically embraced by
EPA for risk assessment of chemical mix-
tures is based on single chemicals.
Demonstrable additivity of mixture
components makes assessing risks much
easier. For example, if two structurally
similar chemicals have similar toxicity
(dose-response) characteristics, it is possible
to regulate a standard for exposure to both as
a mixture based on the toxicity of either
component alone. The presence of chemical
A is not affecting the toxicity of chemical B,
so there's no concern about
possible interactive, or greater
than additive, effects. Exposure
to a mixture with effects pre-
dictable by a linear dose-addi-
tion model would not present a
greater risk than exposure to its
chemical components alone.
When there is predictable
antagonism between chemicals,
or a less additive effect, regula-
tion based on the toxicity risks
of single chemicals can still Jane Ellen
provide adequate protection for velopment c
exposure to a combination of models is a rei
those chemicals. For example,
the presence of one chemical may suppress
the action ofanother.
Risk Overestimation
There are potential problems with the
additivity approach, however. It may great-
ly overestimate the risk when chemicals act
by mechanisms for which additivity
assumptions are invalid. For example,
according to Feron, essential nutrients (vit-
amins, trace elements, essential amino and
fatty acids) possess relatively small margins
ofsafety between the dose people need (the
recommended daily allowance) and the
dose that may be toxic. Consuming these
chemicals simultaneously at their recom-
mended daily allowances would be consid-
ered unhealthy when toxicity of the mix-
ture is assessed on the basis of dose addi-
tion. This of course would not be a valid
assumption, as people routinely take multi-
vitamins and other dietary supplements
with no problem.
A similar problem arises when esti-
mates are made ofexcess cancer risks posed
by exposure to mixtures of chemical car-
cinogens. Animal bioassays are frequently
used to assess carcinogenicity associated
with exposure to individual chemicals.
Statistically derived upper bounds of spe-
Sim
of n
alist
cific exposure levels are gener-
ally used to characterize the
t experimental low-dose risk.
"In the absence of a formal
procedure for calculating
upper bounds for mixtures
under the additivity assump-
tion, regulatory agencies have
adopted the common practice
of assuming upper-bound risk
estimates for individual com-
ponents," says Ralph L.
imons-De- Kodell, National Center for
nechanistic Toxicological Research deputy
ic possibility. director for biometry and risk
assessment. This conservative
approach, which is taken by the FDA on
food additives and by the EPA on haz-
ardous waste-site cleanup, can overstate the
true underlying risk associated with a given
mixture, Kodell says.
Additivity Verified
An assumption ofadditivity often holds up
experimentally and offers support for a
shared underlying mechanism among a
mixture's compounds. A case in point aris-
es from recent findings by HERL and SRI
International, a private research institu-
tion, on the combined effects of paired
ototoxic organic solvents on the auditory
system of rats. William K. Boyse, HERL
chief of neurophysiological toxicology,
explains that the absence ofdata on neuro-
toxicity endpoints for many classes of neu-
rotoxicants was among the reasons for this
investigation. Organic solvents were also
chosen because they are prevalent in haz-
ardous waste sites and because a number of
these compounds cause hearing loss.
The findings of this study were consis-
tent with the EPA default assumption for
noncancer endpoints, says Boyse. "We
could not detect any changes from additiv-
ity. No outcome was predictive ofsuperad-
ditive or subadditive effects. All effects
0
What are we talking about here? The first step in developing an approach to mixtures research is clarify-
ing the terms of the debate. (Source: E.J. Calabrese, Multiple ChemicalInteractions, Boca Raton, FL: Lewis Publishers, 1991.)
Volume 103, Number2, February 1995 143
VA
i
were as predicted by a linear dose-addition
model. The implication is that these oto-
toxic solvents operate through the same or
similar mechanisms."
Risk Underestimation
Recent studies in rodents seem to under-
score the dangers of generally applying the
additivity assumption to risk assessment of
chemical mixtures because the assumption
may lead to an underestimation ofrisk.
Among the studies are several subacute
toxicity studies of a combination of nine
chemicals (including aspirin, cadmium
chloride, stannous chloride, formaldehyde,
and dichloromethane), all of which are
highly relevant to the general human pop-
ulation in terms of use pattern, dose level,
and frequency of exposure. According to
TNO's John Groten, a four-week inhala-
tion study at the no-adverse-effect level
(NAEL) for each of the chemicals revealed
pathological changes in the nose and liver.
"This suggests that combined exposures to
compounds even at their NAEL will not
necessarily result in a NAEL for the combi-
nation," says Groten. He also points out
that, interestingly, even at one-third the
NAEL of the individual chemicals, some
minor adverse effects were found.
"Quantitative risk assessment to a large
degree is still based on assumptions," Kodell
says. "There are a lot of critical assumptions
that go into it that have yet to be verified
biologically. It remains a goal to strive
toward. Still, I don't think you should wait
for all the information before doing some-
thing. That's why EPA and FDA use the
best-available assumptions to produce some
appropriate regulations."
Interactive Mechanisms and
Toxicokinetics
Within organisms, chemicals can interact at
a number of different levels, through
absorption, metabolism, distribution, and
at the site of action. Melvin Anderson, a
toxicologist with ICF Kaiser Systems, says
that it's through studies of pharmacokinet-
ics that we begin to understand the behav-
ior of mixtures. "Unless we learn
what the mechanisms of these
interactions are, we have little
hope of extrapolating to lower
doses and from one species to
another." Rather than refer to
chemical interactions strictly in
terms of additivity, synergy, or
antagonism, Anderson says he
prefers to use the terms "pharma-
cokinetic" and "pharmacodynam-
ic" interactions. Anderson defines
the terms thus: "Pharmacokinetic
interactions are when the tissue Eula Bin
dose of a chemical per unit of should u
exposure is altered by co-exposure data in as
igha
ise
sses
to another chemical. A phar-
macodynamic interaction is
when tissue responses to a
unit concentration of the
chemical is altered due to co-
exposure to other chemicals."
Anderson says that over the
past 20 years, toxicologists
have been heavily cautioned
not to equate responses to
administered dose. "We really
have to know what kind of
chemical gets to the tissues
and in what form, and the Ralph Kode
intensity of exposure, to cor- assumptions.
relate outcome to a particular timate the risk
exposure.
EPA toxicologist Linda Birnbaum
agrees. Her work with toxic equivalency
factors in risk assessment for dioxin and
dioxinlike chemical compounds involves
interactions at a molecular level. "The abil-
ity of a chemical to interact through the
receptor just tells you that it has the ability
to do that; it doesn't tell you what happens
when the chemical actually gets into the
animal," she says. "And in fact, pharmaco-
kinetic factors play a very major role in
tempering the potency of a number of
compounds. PCB-77 has a very good abili-
ty to act with a receptor, but it is metabo-
lized and eliminated almost immediately
upon entering an animal's body."
Through animal models of pharmaco-
kinetic interactions of specific chemical
compounds, scientists hope to make predic-
tions for occupational exposure, leading to
improved regulatory standards for work-
place safety. For example, there are experi-
ments that simulate human exposures to
atmospheric mixtures of styrene and buta-
diene (a probable human carcinogen,
according to the EPA) that may occur dur-
ing processing and production of
styrene-butadiene polymers. In one such
study, toxicologists led by Gyorgy A.
Csanady at GSF-Institut fur Toxikologie in
Neuherberg, Germany, found the amount
of butadiene metabolized was inhibited by
simultaneous exposure to styrene, whereas
butadiene had no detect-
able effect on the kinetics
of styrene. Findings of
_. antagonistic metabolic
_ interactions between these
compounds have also been
reported by toxicologists at
the Chemical Industry
Institute of Toxicology. At
CIIT, inhibition of the
oxidative metabolism of
butadiene as well as inhibi-
tion of further oxidation
r-Scientists and detoxification of an
immunological important reactive metabo-
.sing mixtures. lite has been shown. This
..n
of
metabolite, butadiene monoe-
i poxide, is thought to be partly
responsible for the genotoxicity
ofbutadiene.
An interesting example ofa
chemical mixture with meta-
bolic interactions that pose a
health risk is the interaction
between trichlorethylene (TCE)
and ethanol. TCE is found at
most hazardous waste sites and
is the most common ground-
water contaminant near those
-D efa u It sites. Ethanol both induces and
nay overes- competes with TCE metabo-
lism, and according to M.
Moiz Mumtaz and Jo Ann Freedman ofthe
Agency for Toxic Substances and Disease
Registry, the effect of this interaction
depends on the exposure protocol; that is,
the timing of exposure. The ATSDR scien-
tists found that simultaneous exposure
causes competition for enzymes and co-fac-
tors in TCE metabolism, with consequent
decreased potentiation of TCE-induced
central nervous system depression.
Induction predominates if the interval
between exposure to ethanol and exposure
to TCE is three hours. Simultaneous expo-
sure potentiates cardiac arrhythmias, while
increasing the exposure interval decreases
this potentiation, but increases liver toxici-
ty. "Degreasers flush," is a severe and some-
times fatal intoxication that can occur in
habitual alcohol drinkers exposed to TCE.
Knowledge of antagonism between
compounds may prove useful for reducing
toxicity. Robert Snyder, a professor oftoxi-
cology at Rutgers University, has been
exploring ways to reduce benzene toxicity
by modifying its metabolism in rats
through co-exposure to toluene. Exposure
to high doses of benzene (in excess of 25
parts per million) over prolonged periods
has been associated with the development
of aplastic anemia among workers in the
printing, shoemaking, and plioform (a
saran precursor) industries. "Toluene is a
competitive inhibitor of benzene metabo-
lism," Snyder says. "Toxic metabolites of
benzene aren't produced, so you reduce its
toxicity. Any way you can prevent metabo-
lism, you can protect against the effect."
He points out, however, that while toluene
is antagonistic toward benzene metabo-
lism, the chemicals act additively to pro-
duce central nervous system depression.
Interspecies Extrapolation
In terms of using interspecies extrapolation
as the basis for risk assessment, what holds
true for single-chemical studies seems to
apply with equal vengeance to mixtures.
Zenick points out that equally relevant to
mixtures studies are questions pertaining to
whether mechanisms are homologous
Environmental Health Perspectives144
among species and questions about what
happens to the mixture as a result of phar-
macodynamics. "You are potentially faced
with multiple mechanisms elicited simulta-
neously within each species under consider-
ation; thus the ability to tease out the issue
of mechanisms becomes more complex."
Zenick adds, "Even if you have homolo-
gous mechanisms that are present and in
operation at high doses across species, you
cannot be confident those same mecha-
nisms would be there at low doses more
appropriate to human exposure levels."
Indeed, interspecies differences in
metabolic activation and deactivation of
single compounds are common. It should
not come as a surprise that rats and mice
respond differently to concurrent expo-
sures to certain chemicals. Such is the case
with chloroform and TCE as studied in
mice by HERL's Simmons and David J.
Svendsgaard, along with University of
North Carolina toxicologist Hui-Min
Yang. They note that previous reports have
shown reductions in chloroform-induced
liver and kidney toxicity in rats co-exposed
to chloroform and TCE compared to rats
treated with chloroform alone. In their
more recent study, concurrent oral expo-
sure to chloroform and TCE in mice "sug-
gests synergistic liver toxicity in higher
dose regions and additive toxicity in lower
dose regions." Kidney toxicity, they state,
appeared additive.
Contributions from Epidemiology
"Determining the health risks of complex
mixtures poses equally daunting challenges
to toxicologists using experimental methods
and to epidemiologists using observational
methods," says Jonathan Samet, chair ofthe
Department of Epidemiology at Johns
Hopkins University. "Some of the weak-
nesses ofepidemiologic methods for investi-
gating chemical mixtures are also evident,"
Samet adds. "Exposure assessment may be
particularly challenging. Random and non-
random errors in the estimation of expo-
sures may blunt the sensitivity of epidemio-
logic studies and constrain interpretation of
findings. And large, expensive studies may
be indicated."
"Epidemiologic data have the implicit
strength of directly addressing risks of expo-
sures in human populations and, for this rea-
son, the findings of epidemiological research
have received prominence in the development
ofregulations," Samet says. Samet also points
out that in regard to chemical mixtures, epi-
demiology studies can offer information on
the consequences of community and work-
place exposure to the mixtures present. And
when laboratory replication is not feasible or
even possible, epidemiologic studies can sup-
port study results.
Epidemiological investigations have
proved highly informative for identifying
adverse consequences ofdiverse environmen-
tal exposures, including such chemicals as
benzene and vinyl chloride. Epidemiology
has also been central in identifying adverse
health effects of mixtures such as tobacco
smoke and outdoor and indoor air pollution.
Samet says that such highly variable mixtures
of gaseous and particulate agents have not
been readily investigated using toxicologic
approaches. "Epidemiologic research has
been less informative in characterizing the
effects of exposures to relatively low levels of
mixtures in determining the components of
mixtures that may be most relevant to disease
causation, and in understanding the interac-
tions among components of mixtures," he
says.
Study designs such as the nested case-
control and case-cohort studies involve sam-
pling from populations to enhance feasibility
and reduce costs. These designs should still
yield estimates of effect that are unbiased
and reasonably precise
compared to those ob-
tained by studying the
entire population. New
tools offering promise for
mixtures research include
methods for time-activity
assessment, area and per-
sonal monitoring of expo-
sures, and biomarkers.
Eula Bingham, a pro-
fessor of environmental
health at the University of
Cincinnati, urges a multi-
disciplinary approach to Linda Birnbaum
mixtures research that viewed as an interi
would allow toxicologists
to exploit information from human disease.
"What are some of the diseases that concern
us regarding health effects of mixtures?" she
asks. "Cancer and specific types of
cancer."According to Bingham, there is
immunological evidence that some agents
work together in more ways than additivity.
Examples include radon and smoking and
asbestos and smoking in lung cancer and
alcohol and smoking in pharyngeal cancer.
She says that revised EPA guidelines on
chemical mixtures risk assessment should
emphasize the importance of synergism,
rather than additivity. Bingham suggests
"that we worry about adding up over time,
adding to the immunologic burden, and to
the estrogenic burden."
"Mixtures are tough for everybody,"
Samet says. "Epidemiologic approaches rep-
resent the only way to look at the conse-
quences of mixtures as they are experienced
by people."
New Approaches
An approach to improving the assessment
of potential hazard for complex chemical
i-T
imal
mixtures still under development is the use
of toxic equivalency factors (TEFs). The
use of TEFs involves development of a
potency ranking scheme which relies on
existing data and scientific judgment. The
TEF is derived by observing the data avail-
able for one chemical, by looking at the
dose-response characteristics for that com-
pound, and comparing it to the dose-
response characteristics observed for a
prototypical compound. Thus, each chem-
ical in a mixture has a TEF assigned to it.
Says Birnbaum, "Multiply that fractional
potency value by the mass [ofthe mixture],
sum it all up, and that's the total toxic
equivalency."
Birnbaum says TEFs are used for observ-
ing differences in orders ofmagnitude and that
they are not precise estimates of relative po-
tency. "I think it's important that for risk
assessment this is viewed by at least our agency
[EPA] as an interim approach until we might
develop something that will work better."
What is in the future
for risk assessment of
chemical mixtures? At the
HERL symposium on
chemical mixtures and
risk assessment in Nov-
ember, William Greco of
the Roswell Park Cancer
Institute predicted, "By
the beginning of the next
millennium, routine as-
sessment of the effects of
chemical mixtures, for
both toxic and therapeu-
rEFs should be tic agents, will be very
pproach. different from approaches
commonly used today."
The future paradigm, according to Greco
and Roswell Park's Leonid A. Khinkis, will
include assays that are more automated and
robotized; routine study of multicompo-
nent mixtures; empirical models that will
routinely fit to data with sophisticated user-
friendly software on fast, inexpensive com-
puter workstations; insightful computer-
based graphical exploratory analysis proce-
dures; routine combined pharmacokinet-
ic-pharmacodynamic modeling ofchemical
mixtures, and standardization of nomencla-
ture and approaches. "The seeds of a brave
new world have already been planted, and
spring is approaching," said Greco.
Leslie Lang
Leslie Lang is a freelance journalist in Chapel Hill,
North Carolina.
Volume 103, Number2, February 1995 145
